// a2x: -L 
// a2x: --dblatex-opts "-d"
// a2x: --dblatex-opts "-p ./qvd.xsl"
// a2x: --dblatex-opts "-s ./AdministrationManual.sty"

The QVD Administration Manual
=============================
Rowan Puttergill <rowan.puttergill@qindel.com>
$Id: AdministrationManual.txt 11680 2011-07-22 08:22:43Z rputtergill $
:author initials: RP
:email: rowan.puttergill@qindel.com
:doctype: book
:toc:
:icons:
:numbered:
:qvdversion: 3.1
:website: http://theqvd.com/
ifdef::blogpost[]
// Use the same article for both a blog post and on the AsciiDoc
// website.
:blogpost-status: published
:blogpost-doctype: book
:blogpost-posttype: page
:blogpost-categories: doc
endif::blogpost[]

[preamble]
Revision Information
--------------------
This document was last updated: {revdate}

Its current revision number is set at: {revnumber} 

[preface]
Preface
========
This document will provide you with all of the information that you
need to install, manage and administer any of the QVD components
within a QVD solution. As an open-source product, QVD is constantly
growing and being improved. We endeavour to keep our documentation as
complete as possible and encourage readers to notify us of ways that
the documentation can be improved. If you have any queries or
suggestions, please email us at info@theqvd.com.

The document is broken into three main parts. The first part discusses
the core components that make up a solution, how they interact and how
they are installed and configured. The second part deals with
integration issues and how to tweak behaviours within QVD to achieve
better performance or to be more scalable. The third part is dedicated
to providing you with all of the information that you may need to
create and manage the Operating System Disk Images and Virtual Machines
that get loaded into each virtual desktop.

Additionally, we provide a Bibliography to reference external material
that may help you to gain a better understanding of the different
technologies involved in a QVD solution. We also provide a Glossary of
commonly used terms, that may help you to understand some of our own
terminologies and some less frequently encountered terms when you come
across them.

include::WhatIsQVD.txt[]

Core Components
===============
[partintro]
--
In this part of the manual, we discuss the core components that make
up a QVD Solution. We will explain the architecture of a solution, and
we will cover the installation and configuration settings specific to
each component in detail.
--

Components and Architecture
----------------------------

Introduction to QVD Components
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
QVD {qvdversion} is comprised of a number of core components that work
together to create a complete QVD solution. While not every single
component is necessarily required in order to create a functioning
environment, it is advisable that all components are actually installed to
ensure ease of management and to protect the stability of the platform.

There are three major server side components:

* QVD server,
* Administration Server, and
* PostgreSQL DBMS.

Ideally, each of these should be stored on a dedicated host for
stability reasons, although it will become clear later that these
components will have access to some shared resources in order to
function properly.

While it is likely that you will only have one Administration Server
and one PostgreSQL Database system in your environment, it is possible
to have any number of QVD Server Nodes.

Finally there is a client side component:

* QVD GUI Client

The client is packaged for a variety of Linux base operating systems,
and for Microsoft Windows. Development for an Android client is also
underway and this should be available in the next major release of the
software.

The client software can be installed on as many host systems as
required.

include::HighLevelArchitectureDiagrams.txt[]

QVD Server Architecture
~~~~~~~~~~~~~~~~~~~~~~~
[[qvd_node_architecture]]
In most production environments, the architecture of a QVD environment
is such that several QVD Server Nodes will be running in parallel to
each other. The QVD environment is designed to handle a fully
load-balanced environment, so that you can have a High Availability
solution.

Internal Elements
^^^^^^^^^^^^^^^^^
A QVD Server Node is composed of two core elements:

* *L7R* - A Layer-7 Router that acts as the broker within the server
  environment, responsible for authenticating users, establishing
  sessions and routing connections to the appropriate virtual IP
  addresses. In general, the L7R is responsible for managing user
  status.
* *HKD* - A 'House Keeping Daemon' that tracks the status of virtual 
  machines. The HKD is responsible for starting and stopping virtual
  machines. The HKD monitors the health of each virtual machine and 
  then updates status information within the QVD Database, so that 
  other Nodes and the administration tools are able to function 
  accordingly. In general, the HKD is responsible for managing virtual
  machine status.

QVD Node Behaviour
^^^^^^^^^^^^^^^^^^
The QVD Node is a daemon that runs on the QVD Node Server with the
purpose of managing the L7R and HKD daemon processes. Its primary
function is to start and stop these processes when it receives the
appropriate command or when it detects that there is some anomaly that
may cause the QVD Server Node to fail.

The QVD Node, ensures that networking elements required for virtual
machine connectivity are all in place and available. This means that
QVD Node will ensure that DHCP is configured and running.

The QVD Node also looks for broken database connections, to determine
if there is a problem with the HKD, and will automatically kill and
restart the HKD if there seems to be some problem with the HKD's
communication with the QVD Database.

Logically, the QVD Node performs functions that are closely related to
the HKD and in the future these two processes are likely to be merged.

HKD Behaviour
^^^^^^^^^^^^^
The 'House Keeping Daemon' is responsible for managing virtual
machine states based on information that it detects within the QVD
Database. The HKD regularly polls the QVD database to determine the
status of each Virtual Machine. If the status has been changed by an
external element (such as the L7R or the QVD-WAT) the HKD is
responsible for enacting the appropriate commands to effect the status
change.

When QVD is configured for KVM virtualization, the HKD runs a KVM instance 
for each virtual machine that needs to be started, and provides startup 
options based on information obtained from the database.

When QVD is configured for LXC virtualization, the HKD will first check to
determine whether the image file has been uncompressed into the basefs folder
in the shared storage area, and uncompresses the image file if this has not
already been done. The HKD then uses the 'fuse-unionfs' module to perform a
union mount of the image in the basefs folder with an automatically generated
overlay file system and home file system. This mount is performed inside the
rootfs folder in the shared storage. Finally, the HKD will load the newly mounted
image into an LXC instance.

As the Virtual Machine instance starts, the HKD will check that the image boots 
correctly, that it has network connectivity and that the QVD-VMA is running within 
the virtual machine. If any of these checks fails, the HKD will change the state of 
the virtual machine to 'blocked' within the QVD Database. After a short period, the 
HKD will kill the running virtual machine.

During each loop run that the HKD performs it will check the health
of all running virtual machines, it checks the database to determine
if there are any VM state changes, implements any changes to VM state,
and updates information in the database pertaining to VM state.

QVD Client and L7R Server Node Interactions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The QVD Client connects directly to the L7R element of a QVD Server
Node. The Client initiates a connection over HTTPS, where it is
prompted to provide HTTP BASIC authentication credentials.路

The L7R will then connect to the backend database to determine how
authentication should take place (i.e. locally or using an external
LDAP directory) and take the appropriate steps to perform the
authentication process. The L7R will return an HTTP OK response if the
authentication was successful, or will return a 401 Unauthorized if
authentication fails.

Once authenticated, the client requests a list of virtual machines that路
are available to the user. The server responds with a JSON formatted list路
of virtual machine IDs and their corresponding status. The client selects
an appropriate virtual machine to connect to and submits a GET request
with the ID of the virtual machine at a standard GET variable. It also
requests a protocol upgrade to QVD/1.0 within the HTTP request
headers.

The L7R performs the necessary steps to ensure that the virtual machine路
is up and waiting for connections using the NX protocol. If the
virtual machine is not running on any server node, it will determine
which node it should be started on and automatically start a virtual
machine for that user. In all events, the L7R will determine which
node is running the virtual machine and will forward all requests to
this machine for all further handling, including checking to see that
an NX session can be set up. During this process, the L7R will return a series 
of HTTP 102 responses indicating the progress of the processing required to 
establish a connection with the Virtual Machine. If the virtual machine is 
available, the L7R establishes a connection to the *nxagent* running on the 
virtual machine and becomes a pass-thru proxy for the NX session. Once the 
session is set up, the L7R will issue a final HTTP 101 (Switching Protocols) 
response to the client, and the protocol for all future interactions with the 
client will be upgraded to the NX protocol, secured using SSL. The L7R
updates the QVD Database to set the status for the virtual machine to
indicate that a client is connected.

From this point onward, all communications between the client and the
Virtual Machine are performed over the NX protocol via the L7R. When
the client disconnects, the L7R updates the QVD Database to represent路
the change in virtual machine status.

The process flow is indicated in the following diagram:

.Protocols and process flow for Client/Server Node interaction
image::../images/client_server_protocol.png[alt="Protocols and process flow for Client/Server Node interaction" width=98%]

L7R in a HA load-balanced environment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
As already mentioned, Server Nodes are designed for a fully
load-balanced environment. In order to cater for this, the L7R element
of each Server Node is capable of redirecting traffic for a particular
virtual machine to any other Server Node in the environment.

The usual configuration is such that a Virtual Machine is started for
each user on any one of the server nodes. When a user is authenticated
by any L7R within the solution, the L7R determines which server node
is currently running a virtual machine for the authenticated user.
This is achieved by querying the QVD Database. If a running virtual
machine is detected within the environment, the L7R will reroute all
traffic for that connection to the appropriate server node.

If no virtual machine is currently running for the user, the L7R makes
use of an internal algorithm to determine the most appropriate node to
start a new virtual machine for the user. This algorithm is based on
assessing which node has the highest quantity of free resources,
calculated as the weighted sum of free RAM, unused CPU, and a
random number to bring some entropy to the result. 

When an appropriate node has been selected, the database is updated so
that a virtual machine will be started by the HKD on the correct host. The 
L7R will then reroute all traffic for that connection to the server node 
that has been selected to run the new virtual machine.

Virtualization Technologies
^^^^^^^^^^^^^^^^^^^^^^^^^^^
QVD supports two different virtualization technologies: KVM (Kernel Virtual Machine)
and LXC (Linux Containers). Each virtualization technology comes with its own set
of advantages and will prove more useful for particular use cases. Therefore, a good
understanding of your own requirements and an understanding of these two technologies
will help you to determine how to configure your QVD deployment.

KVM Virtualization
++++++++++++++++++

The Kernel Virtual Machine (KVM) is a fully featured hypervisor 
that runs inside of the kernel of the linux host operating system. The hypervisor ensures
absolute separation from the underlying host operating system, allowing you to load completely
different operating systems and distributions into each virtual machine and expect them to
function just as if they were running on completely separate hardware.

While there is some debate over whether KVM is actually a Type-1 bare-metal hypervisor, since it does require
the linux Kernel in order to function, most virtualization experts agree that combined with 
the linux Kernel, KVM functions in exactly the same way as any other bare-metal hypervisor,
such as Xen or VMware's ESXi. In fact, in the recently published SPECvirt 2011 benchmark reports, KVM
came second in performance only to VMWare ESX, indicating a high level of viability as a commercial-grade
virtualization platform.

Since KVM uses absolute separation, it is much easier to configure and manage than LXC. However,
although it offers competitive performance to other hardware hypervisors, each virtual machine is
necessarily running its own kernel. Resources need to be dedicated to each virtual machine, whether they
are being used or not. In this way, KVM is not as efficient as LXC, but offers much greater flexibility
and ease of management.

LXC Virtualization
++++++++++++++++++

Linux Containers (LXC) provide Operating system-level virtualization.
In this way, they act as an alternative to the full hardware-level
virtualization provided by the KVM hypervisor. LXC behaves in a
similar manner to a chrooted environment within Linux, but offers a
greater level of isolation and management of resources between 
containers through the use of 'namespaces' and 'cgroups'. For
instance process IDs (PIDs), network resources and mounts for each
container will be isolated from other containers and can be logically
grouped together to apply resource management rules and other policies
specific to the container. This allows you to gain many virtualization
benefits while keeping down overall resource requirements and by
re-using the same kernel across virtual machines. LXC is fully
supported by the Linux Kernel, and has been included in QVD since
version 3.1.

Virtual Machines and VMA
^^^^^^^^^^^^^^^^^^^^^^^^
Virtual Machines are started by the HKD on a per-user basis. In a
production environment, it is usual for there to be a number of
different QVD Node Servers running in parallel. Virtual Machines are
started for different users across the different QVD Server Nodes, so
that there is one virtual machine instance running for each user that
needs to be provisioned. If a virtual machine has not been started for
a user, and the user connects and authenticates against an L7R, the
L7R will use its load-balancing algorithm to determine which node
should run the user's virtual machine and the database will be updated
so that the virtual machine will be started on the appropriate node.

When Virtual Machines are started, they load an "Operating System Flavour" 
or OSF. The parameters for the Virtual Machine are determined by data stored 
within the QVD-DB for each OSF. In general, the OSF's Disk Image is loaded 
from a network share. There is a separate chapter within this document dedicated 
to creating, editing and managing OSFs.

Virtual Machines make use of _overlays_ in order to best utilize
different elements of the Guest operating system, and in order to make
particular elements persistent. For instance, while write activity is
not persistent within the actual OSF that is loaded, it is important
that data written to the user's home folder or desktop is stored for
future connections to the virtual desktop. 

Within instances of QVD that make use of KVM virtualization, this is achieved
by storing the user's home directory within a 'qcow2' image. This is loaded 
over the home directory within the OSF that is running in the Virtual Machine. 

In instances of QVD that make use of LXC virtualization, this is achieved by
taking advantage of _unionfs_ mounts. The user's home data and any overlay
data is stored within a separate directory outside of the image used for a
virtual machine. These folders can then be mounted over the base image at
runtime, in order to create a container specific to each user and virtual
machine.

The 'qcow2' image or user home directory is usually stored on a network share, 
so that it is accessible  to any server node within the environment. If the user's 
virtual machine is later started on a different Server Node, the user's home directory 
can be loaded at run time and the user's modified data will always be available to the
user. 'Overlays' can also be used to make other data such as log and tmp 
files persistent from a user perspective.

Depending on the virtualization technology configured within QVD, virtual machines
will be started either using KVM or LXC. However, it is important to understand
that the images for these two different technologies are very different and it
is not possible to switch between virtualization technologies.

Once running, each Virtual Machine must load the QVD-VMA (Virtual Machine Agent) in
order to function properly. The VMA will ensure that the *nxagent* is
available so that a client is able to connect to the virtual desktop
that is created for the user. It also returns different states that
helps the L7R to determine user state, which can be fed back to the
QVD-DB. When an OSF is created, it if fundamentally important that the
QVD-VMA is installed and configured in order for QVD to work at all.

QVD Administration
~~~~~~~~~~~~~~~~~~
QVD Administration can be performed using one of two tools:

* *qvdadmin.pl*: a command line utility that can be installed on any
  machine that has the connectivity to the QVD-DB and that is
  configured appropriately for this purpose.

* *QVD-WAT*: a Web-based Administration Tool that allows an
  Administrator to remotely access the solution and to perform a
  variety of administrative tasks using a standard web browser.

Both tools require access to the QVD-DB and will need to be configured
for this purpose. Nearly all of the commands that can be performed
through either of these tools will simply change values for entities
within the QVD-DB. Actions are then carried out by the various QVD
Server Node elements based on the changes made within the QVD-DB.

The QVD Administration tools are also be used to load new images into
QVD and to configure their runtime parameters. In order to facilitate
this functionality, these tools need access to the folders where these
images are stored and accessed by the Virtual Server Nodes. Usually,
this access is provisioned over a network file share such as NFS.

Base QVD Configuration
----------------------
include::QVD_CONFIG.txt[]

QVD-DB
------
include::QVD_DB.txt[]

QVD Web Administration Tool
---------------------------
include::QVD_WAT.txt[]

QVD Server Nodes
----------------
include::QVD_NODE.txt[]

QVD CLI Administration Utility
------------------------------
include::QVD_CLI.txt[]

QVD GUI Client
--------------
include::QVD_CLIENT.txt[]


Design Considerations and Integration
=====================================
[partintro]
--
In this part of the manual, we explore things that will affect the
design of your solution, such as your virtualization technologies, 
storage requirements and authentication mechanisms.
--

Shared Storage
--------------
Since there are multiple server-side components within the QVD
infrastructure, and each of these will usually be installed on a
number of different physical systems, it is important to set up some
shared storage facility that is accessible to all of the hosts within
your server farm.

The currently supported network file sharing services are GFS or OCFS2 
on top of some SAN server (i.e. iSCSI, AoE, etc.) and NFS.

QVD usually keeps all commonly used files in the directory location:

    /var/lib/qvd/storage

NOTE: All of the paths used by QVD are configurable items, so you
should keep in mind that although this is the default location, the
pathnames within your infrastructure may be different depending on
your configuration. You can check these configuration settings using 
the QVD CLI Administration Utility.

While some of the components do not need access to all of the QVD
Storage folders, and in some cases you can opt to have some of these
folders running locally on one system, we recommend that all of these
folders are accessible within some form of network based shared storage.

Storage Folders
~~~~~~~~~~~~~~~
There are a variety of folders that belong in the storage path. Many of
these are specific to the type of virtualization that you choose to make
use of within your environment.

General Storage
^^^^^^^^^^^^^^^

* *staging*: temporary location for all DIs that you want available in
  the QVD-WAT for the purpose of loading as an image. Files located
  here are available within QVD-WAT when you select to add an image.
  The image file will be copied out of this directory and into the
  *images* folder when it is enabled using one of the administration tools. 
  The staging folder can either be hosted locally or on a network share, 
  but must be accessible to the QVD-WAT.
* *images*: location of the DIs (Disk Images) that are
  loaded by the nodes for each Virtual Machine that is created. These
  need to be accessible to QVD Server Nodes and to the QVD-WAT. This
  directory might be stored on a network share, but in a very simple
  configuration where the QVD-WAT is either not used or is hosted on
  the same system as the QVD Server Node, it can be hosted locally
  which will help to improve performance. Note that where KVM 
  virtualization is used, the image is loaded into the virtual machine
  from this directory. When LXC virtualization is used, the image is
  extracted from this directory into the *basefs* directory, before it
  is loaded.


KVM Storage Directories
^^^^^^^^^^^^^^^^^^^^^^^

* *homes*: location of user home data. Under KVM, home data
  is stored in individual files as qcow2 images. The *homes* directory 
  should be accessible to all QVD Server Nodes usually on some type of 
  network file share such as NFS, OCFS2 or GFS2.
* *overlays*: location used to store overlays for data that is constantly
  written to the Operating System in order for it to function correctly, such
  as temporary files and variable data etc. Usually this folder can be hosted 
  locally, but for more persistent behaviour in your virtual machines, you can 
  choose to store these on a network share and configure QVD to make your 
  virtual machines persistent.

LXC Storage Directories
^^^^^^^^^^^^^^^^^^^^^^^

* *basefs*:  location of the DIs (Disk Images) that are
  loaded by the nodes for each Virtual Machine that is created. These
  need to be accessible to QVD Server Nodes and to the QVD-WAT. This
  directory might be stored on an network share, but in a very simple
  configuration where the QVD-WAT is either not used or is hosted on
  the same system as the QVD Server Node, it can be hosted locally
  which will help to improve performance. The basefs folder will contain
  a subdirectory for each DI, which will in turn contain the complete 
  filesystem tree for a functioning operating system
* *homesfs*: location of user home data. Under LXC, home data
  is stored within subdirectories inside the *homesfs* directory, named 
  according to the user-id and the osf-id stored within the QVD-DB. 
  The *homesfs* directory should be accessible to all QVD Server Nodes 
  usually on some type of network file share such as NFS, OCFS2 or GFS2.
* *overlayfs*: location used to store overlays for data that is constantly
  written to the Operating System in order for it to function correctly, such
  as temporary files and variable data etc. Usually this folder can be hosted 
  locally, but for more persistent behaviour in your virtual machines, you can 
  choose to store these on a network share and configure QVD to make your 
  virtual machines persistent.
* *rootfs*: location of the running LXC once all required mountpoints have
  been mounted and configured. Usually this folder is local to each QVD
  Node Server, for performance, but it could equally be stored within the
  shared storage space.


NFS
~~~
In this section of the document we will provide instructions for setting up NFS for QVD, as
this is one of the more commonly used network file sharing protocols used to facilitate shared
storage. We will assume that you are installing and configuring NFS on Ubuntu 10.04 (Lucid Lynx) 
in order to maintain consistency in our instruction set, however you should be able to extrapolate these
instructions to provide NFS access for any distribution.

TIP: We recommend that you run through the following process before
installing any QVD Server components to ensure that when the QVD
components are installed, they are automatically making use of the NFS
share from the beginning. This way, you are less likely to run into
trouble migrating files and creating directories in the longer term.

Installing the NFS Server
^^^^^^^^^^^^^^^^^^^^^^^^^

First install the NFS Server:

----
# apt-get install nfs-kernel-server
----

Configuring the NFS Server
^^^^^^^^^^^^^^^^^^^^^^^^^^

Add an entry to /etc/exports as follows:

----
/var/lib/exports        *(rw,sync,no_subtree_check,no_root_squash)
----

Note that this would mean that on your NFS Server, you would set up
each of the QVD storage directories within the path
`/var/lib/exports`. You can choose an appropriate location if you
would prefer to host these files at an alternative path.

Once you have added your path entry within the NFS Server's exports,
you should reload the NFS Server.

----
# /etc/init.d/nfs-kernel-server reload
----

The NFS Server should now be making the configured path available over
the network.

Mounting the NFS directory on QVD Hosts
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Each host system that is running any QVD server component will now
need to be configured to access the NFS share that we have configured
on the NFS server.

First, create the mountpoint on your host systems:

----
# mkdir -p /var/lib/qvd/storage
----

Ensure that you have the tools required to access an NFS share
installed on your host systems:

----
# apt-get install nfs-common
----

To ensure that the NFS file system is always mounted at boot time,
edit your `/etc/fstab` to add the following line:

----
aguila:/var/lib/exports /var/lib/qvd/storage      nfs rw,soft,intr,rsize=8192,wsize=8192  0       0
----

Note that in the line above 'aguila' is the name of the server hosting
the NFS Share. You should substitute this with the IP address or
resolvable hostname of your NFS Server.


Once you have finished editing your fstab, you should be able to mount
the NFS export on your host systems:

----
# mount /var/lib/qvd/storage
----

Finally, you should check that the NFS export has been properly
mounted. You can do this by running the `mount` command and then
checking the output to see that your NFS export is listed:

----
# mount
...
aguila:/var/lib/exports on /var/lib/qvd/storage type nfs (rw,soft,intr,rsize=8192,wsize=8192,addr=172.20.64.22)
----

LXC Virtualization inside QVD
-----------------------------
include::UsingLXC.txt[]

Authentication
--------------
Although QVD does provide its own authentication framework, which
stores its users within the QVD database, it is quite common to
require integration with another authentication framework so that
changes to user passwords etc, do not need to be replicated within
the QVD-DB.

QVD does provide a certain level of integration with external
resources. In this chapter, we will explore two of the more common
integration requirements, and the level of support offered by QVD for
these authentication frameworks.

LDAP Integration
~~~~~~~~~~~~~~~~
The most commonly used authentication framework is LDAP, and QVD
provides support to authenticate against LDAP right out of the box.

Configuring QVD for LDAP Authentication
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
LDAP Authentication can be configured within QVD by setting a few
configuration keys in the QVD database. This can be achieved using the
<<qvd-admin-cli,QVD CLI Administration Utility>>.

----
# qvd-admin.pl config set l7r.auth.mode=ldap
# qvd-admin.pl config set l7r.auth.ldap.host=aguila:3389 
# qvd-admin.pl config set l7r.auth.ldap.base=dc=example,dc=com
----

In the above example, we have changed the QVD authentication mode to
LDAP, we have set the LDAP Host and port number to `aquila` on port
`3389`. And we have set the LDAP base DN that should be searched for
matching users to `dc=example,dc=com`.

With these basic configuration elements, QVD will automatically search
the LDAP directory for a matching username, and then perform a BIND
against that user using the credentials supplied by the client.
By default, the search is performed with a scope set to 'base' and the
filter set to '(uid=%u)'. Using our example host above, a client
connecting with the Username set to 'guest' would need a corresponding
entry 'uid=guest,dc=example,dc=com' within the LDAP server running on
host 'aquila' available on port '3389'.

It is possible to change the scope and filter settings for the search,
to allow QVD to scan other branches and attributes to find a matching
user:

----
# qvd-admin.pl config set l7r.auth.ldap.scope=sub
# qvd-admin.pl config set l7r.auth.ldap.filter=(|(uid=%u)(cn=%u))
----

The above examples change the default search scope for LDAP
authentication to 'sub' and the filter will cause the search to match
users with either the 'uid' or the 'cn' equal to the provided
Username.

QVD LDAP Limitations
^^^^^^^^^^^^^^^^^^^^
While it is relatively trivial to get QVD to authenticate users
against LDAP, you will still need to create matching users within your
QVD-DB in order to assign virtual machines to them. Currently, the QVD
does not come with any tools for you to do this automatically.
However, customers who choose to pay for support can be provided with
scripts that can cater for batch-mode provisioning of users within
QVD.

QVD tools allowing you to change user passwords within QVD will not
update passwords within an LDAP backend, as this may affect the
functioning of other facilities within your infrastructure. While
these tools will report success for a password change, it is important
to understand that the password that has been changed is the one
stored within QVD-DB for the user, and not the password within the
LDAP directory. If you are making use of LDAP Authentication, all
password changes should be made using the tools that you usually make
use of to manage your users.

OpenSSO and OpenAM
~~~~~~~~~~~~~~~~~~
QVD is capable of providing support for other authentication platforms
such as the federation and access management framework developed by
Sun Microsystems and known as OpenSSO.

NOTE: Since Oracle's acquisition of Sun in 2010, OpenSSO has been
discontinued, but a fork known as OpenAM is available from ForgeRock.

In order for OpenSSO or OpenAM to function within QVD, the appropriate
plugin should be installed onto all of your QVD Server Nodes and you
will need to configure the VMA within each of your OSFs where you want 
to provide federation support.

Currently, this is an advanced configuration option that will require
some professional help to implement. Customers who choose to pay for
support can be provided with instructions and help deploying this
facility.

Load Balancing
--------------

Introduction
~~~~~~~~~~~~
QVD is designed to be used in a load-balanced environment. Since a
typical deployment makes use of several QVD Server Nodes to run all of
the virtual machines, it is common to have Clients connect to these
through a hardware load balancer.Since a virtual machine could run on 
any single Server Node, and a client could connect to any other Server Node, 
QVD's L7R component will handle the forwarding of a connection to the correct 
Server Node. However, since each Server Node has limited resources,
running virtual machines need to be equitably distributed to maximize
system resources across the Server Node cluster.

If a Virtual Machine is not already running for a connecting user, the
L7R component will determine which Server Node would be most
appropriate to use in order to start a new virtual machine for that
user. QVD uses its own load balancing algorithm to determine which
node should be used for the new virtual machine. This algorithm
assesses which node has the highest quantity of free resources,
calculated as the weighted sum of free RAM, the weighted sum of unused
CPU, and a weighted random value to bring some entropy to the result.
Once the best candidate Server Node has been selected, the QVD-DB is
updated to indicate that the virtual machine should be started on this
Server Node, and the virtual machine is automatically started by the
Server Node's HKD.

This whole process is known as QVD Load Balancing, and it is used to
ensure that running virtual machines are equitably distributed across all of
the Server Nodes. This maximizes the resources available to any
virtual machine to preserve healthy functionality.

Changing the weighting in the QVD Load Balancer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The default QVD Load Balancing algorithm calculates the current system
load for each of the available server nodes by multiplying the
available RAM, available CPU and a random number in order to score each
system in the cluster. As we have already mentioned, these figures are
weighted, so that you can alter how the load balancer functions.

Increasing the weight on the RAM variable in the algorithm will cause
the load balancer to add precedence to systems with more available
RAM.

Increasing the weight on the CPU variable in the algorithm will cause
the load balancer to add precedence to systems with more available 
CPU.

Increasing the weight on the random variable in the algorithm will cause
the load balancer to increase the likelihood that a more random server
node will be selected.

These weights are controlled as configuration settings within QVD-DB,
and can be altered using the QVD CLI Administration Utility:

----
# qvd-admin.pl config set l7r.loadbalancer.plugin.default.weight.cpu=3
# qvd-admin.pl config set l7r.loadbalancer.plugin.default.weight.ram=2
# qvd-admin.pl config set l7r.loadbalancer.plugin.default.weight.random=1
----

In the above example, we have assigned more weight to CPU resources,
slightly less to RAM, and even less to the randomizer. This will
result in new virtual machines being started on the Server Nodes that
tend to have more CPU resources available.

Building a Custom QVD Load Balancer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Since QVD is an open-source product built largely in Perl, it is
relatively simple to build your own customized QVD Load Balancer that
uses an alternate algorithm. A typical use case would be where you
have a dedicated set of Server Nodes that you would prefer to use over
another set.

QVD has a plugin system for load balancers. A load balancer plugin is
a subclass of QVD::L7R::LoadBalancer::Plugin that has to be within the
package QVD::L7R::LoadBalancer::Plugin.

Plugin API
^^^^^^^^^^

get_free_host($vm) = $host_id
+++++++++++++++++++++++++++++

Return the id of the node on which the virtual machine $vm should be
started. A load balancer has to implement at least this method.

The parameter $vm is QVD::DB::Result::VM object. It gives you access
to the virtual machine's attributes and properties. The attributes and
properties of the VM's user and OSF can be accessed through $vm->user
and $vm->osf respectively. Other data can be accessed through QVD::DB.

init()
++++++

Initialize the load balancer. Use this if your load balancer has to be
step up, for example by loading a persistent cache.

Minimal example: random assignment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This load balancer assigns virtual machines to random backend nodes.

[source,perl]
----
package QVD::L7R::LoadBalancer::Plugin::Random;

use QVD::DB::Simple;
use parent 'QVD::L7R::LoadBalancer::Plugin';

sub get_free_host {
    my ($self, $vm) = @_;
    my $conditions = { backend => 'true',
                       blocked => 'false',
                       state   => 'running' };

    my $attr = { columns  => 'host_id' };

    my @hosts = rs(Host)->search_related('runtime', $conditions, $attr)->all;
    return $hosts[rand @hosts]->host_id;
}

1;
----

Operating System Flavours and Virtual Machines
==============================================
[[osf_and_vm]]
[partintro]
--
In this part of the manual, you will find all of the information that
you need to create, edit and manage an Operating System Flavour (OSF)
that will get loaded into your Virtual Machines. We also explore the
Virtual Machine Agent in a little more detail to see how you can use
it to trigger your own functionalities based on actions performed by
users accessing the Virtual Machine.
--

DI Creation
-----------

Introduction
~~~~~~~~~~~~
An OSF (Operating System Flavour) is actually composed of two elements:
a DI (Disk Image), and some runtime parameters stored within the QVD-DB
when the Disk Image is loaded into QVD using QVD-WAT or the QVD CLI
Administration Utility. In this chapter, we will concern ourselves
largely with the actual Disk Image part of the OSF, since the runtime
parameters are covered in the other relevant chapters.

QVD uses DIs in order to serve groups of users that make use of a
common set of applications. By using a single image to cater to a
number of users, it becomes easier to administer desktop environments
for all of your users. It also improves overall security, since a
policy can be applied to each group of users.

In this way, if a group of users require a particular application, you
can install it once and the change will apply to all of the users that
share the same DI. Equally, you can remove an application from an
entire group's desktop environment.

DIs can easily be duplicated, so that you can quickly create
additional environments for different subsets of users. By copying a
base image, you can edit the copy and provide additional applications
or other customizations to a second set of users without having to repeat 
a full operating system installation.

In this way QVD can massively reduce administration and maintenance,
improve desktop conformity, and ease security policy implementation.

In this chapter, we will look at the process involved in creating your
own base Disk Image for an OSF, so that you can implement an
environment that is perfectly suited to your users.

System Requirements
~~~~~~~~~~~~~~~~~~~
You can create a DI that can be loaded into QVD on any Linux system
that you have available for the task. In order to create an image, you
will need to meet the following base requirements:

* Server x86 with virtualization extensions (Intel or AMD).
* Linux Operating System (preferably Ubuntu, in order to conform with
  these instructions)
* At least 10 GB free disk space
* qemu-kvm installed
* An Ubuntu Desktop installation ISO to be used for your Guest
  operating system

Creating the Image
~~~~~~~~~~~~~~~~~~
In order to create your DI, you will need to download the installer
of the guest operating system that you intend to serve to your users.
Since the DI will need to support the QVD VMA we recommend that you
use an Ubuntu variant as your choice of operating system, since it
will prove easier for you to set up and configure this environment.
It is possible to make use of any other Linux-based operating system
but you may need professional help creating a functional image.

For customized packages, we recommend that you install Ubuntu using
the 'Desktop alternate installer'. You will need to visit
http://www.ubuntu.com/download/ubuntu/alternative-download in order to
find the installation ISO that you should download.

Create a QCOW2 File
^^^^^^^^^^^^^^^^^^^
QVD will make use of the qcow2 disk image format to create a
virtual disk that will be used to store the DI. This virtual disk
file is essentially the base hard disk used within each virtual machine.
In order to create the qcow2 disk image file, you can run the
following command:

----
# kvm-img create -f qcow2 example.img 4G
----

In the command, the file that will be created will be called
`example.img` and will have a virtual hard disk size of a maximum of 4GB. In
actuality, the qcow2 will only create a relatively small image file.
One of the features of the qcow2 format is that it can expand the
image as required. the 4GB limit is applied to prevent the image from
growing too large without any control.

TIP: Remember that user's home directories will not be stored within this
image, so the limit to the image file size will not affect user home
space.

Installing the Operating System
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Once you have created the virtual disk file that you will install the
base Operating System into, you need to load it into a virtual machine
and boot the Ubuntu CD installer image within the virtual machine. You
can do this by running `kvm` in the following way:

----
kvm -hda example.img -cdrom ubuntu-10.10-alternate-i386.iso -m 512
----

In the example command above, the `ubuntu-10.10-alternate-i386.iso` is
the installer ISO that you would have downloaded from the Ubuntu
website.

KVM should load a basic virtual machine that will boot the installer
ISO. You should be able to follow the instructions in the installer to
complete a standard Ubuntu installation within the virtual machine.
You will notice that you are installing into the virtual hard disk
which is only 4GB in size. During the installation, you will be
prompted for a username and password. You should select a username and
password that you will use to manage package installation and other
administrative tasks within the DI at a future date.

When you have completed the installation, the installer will prompt
you to reboot. You should allow the installer to reboot and KVM will
restart the virtual machine, this time booting off the virtual hard
disk and loading your newly installed Operating System.

include::QVD_VMA_CONFIG.txt[]

Editing a DI
------------
At any point, you are able to quickly edit a DI. This means that you
can add new applications or remove existing applicaitons, or you can
implement new policies.

In order to edit an existing image, ensure that no users are
connected and using the image that you want to edit. Stop any Virtual
Machines that are currently making use of the DI, and block access to
them so that no users can connect and start up a virtual machine while
you are working on its underlying image.

Once the virtual machines have all been stopped, locate the DI file
that you wish to edit and run it within KVM in the following way:

----
# kvm -hda example.img -m 512
----

KVM will load a virtual machine and allow you to login as the user that 
you created when you installed the Operating System. You can now perform any
administration tasks as this user.

When you have completed any work on the DI, shut it down. You can
mark the virtual machines that require access to the image as
'unblocked' and allow them to start up again.

VMA Hooks
---------
[[qvd-vma-hooks]]

Introduction
~~~~~~~~~~~~
VMA Hooks can be configured within an DI to trigger functionality within a Virtual Machine
when particular QVD related events take place. This allows you to automatically modify platform
behaviour to adapt the operating system to address particular client-related requirements and
to solve concrete problems.

Hooks are added as configuration entries within the VMA configuration file on the underlying DI.
Therefore, by editing the `/etc/qvd/vma.conf` file and adding an entry similar to the following:

....
  vma.on_action.connect = /etc/qvd/hooks/connect.sh
....

it is possible to ensure that the script `/etc/qvd/hooks/connect.sh` running on the virtual machine
will be executed everytime that a user connects to the virtual machine.

It is also possible for QVD to provision scripts with command line parameters that are specific to QVD,
such as:

  - State changes, actions, or the provisioning process that has triggered the call to the hook. 

  - Virtual machine properties defined in the administration database

  - Parameters generated by the authentication plugins. 

  - User connection parameters.

  - Parameters supplied by the client program. 

Hooks have their own log file, stored within `/var/log/qvd/qvd-hooks.log` on the virtual machine. This makes
it possible to view which hooks have triggered scripts to run and to debug any unusual behaviours.

Action Hooks
~~~~~~~~~~~~
Action Hooks are executed every time that a particular action begins.

If the hook fails with a non-zero error code, the action will be aborted. 

All action hooks receive these parameters. 

  - _qvd.vm.session.state_: Current X-Windows server state

  - _qvd.hook.on_action_: Action that triggers the hook.


connect
^^^^^^^
key: *vma.on_action.connect*

This hook is executed when a user starts (or resumes) an X-Windows session using the QVD Client. The script will execute
after all Provisioning Hooks have been triggered.

It also receives the following parameters by default:

  - _qvd.vm.user.name_ : the user's login. 

  - _qvd.vm.user.groups_ : groups that the user belongs to. 

  - _qvd.vm.user.home_ : the user's directory home. 

This hook is capable of receiving other connection parameters and any additional parameters assigned to the VM within the QVD-DB.

pre-connect
^^^^^^^^^^^
key: *vma.on_action.pre-connect*

This hook is executed when a user starts (or resumes) an X-Windows session from the QVD Client, with the difference 
that it will trigger a script to execute _before_ any of the Provisioning Hooks are implemented.

Parameters for pre-connect are the same as that for connect.

stop
^^^^
key: *vma.on_action.stop*

This hook is executed when an X-Windows session receives a request to be closed. This behaviour usually occurs when the VMA
recieves such a request from the QVD-WAT or the QVD CLI Administration Utility.

There are no additional parameters for this hook.

suspend
^^^^^^^
key: *vma.on_action.suspend*

This hook is executed when an X-Windows session is suspended. This usually happens if a user closes the QVD Client application.

There are no additional parameters for this hook.

poweroff
^^^^^^^^
key: *vma.on_action.poweroff*

This hook is executed when the virtual machine is shut down. 

There are no additional parameters for this hook.

State Hooks
~~~~~~~~~~~
State Hooks are executed when changes within the X-Windows session take place. These hooks
will always receive the parameter _qvd.hook.on_state_ with the current X-Windows state. 

connected
^^^^^^^^^
key: *vma.on_state.connected*

This hook is executed once a connection has been succefully established between the QVD Client 
and the X-Windows server that runs in the virtual machine. 

suspended
^^^^^^^^^
key: *vma.on_state.suspended*

This hook executes once the user closes the QVD Client and the X-Windows session is in the 
suspended state.

stopped
^^^^^^^
key: *vma.on_state.disconnected*

This hook executes when the X-Windows session ends.

Provisioning Hooks
~~~~~~~~~~~~~~~~~~
Provisioning Hooks recieve the same parameters that are available to the 'connect' Action Hook.

add_user
^^^^^^^^
key: *vma.on_provisioning.add_user*

When a user is connected for first time, if the user still does not exist, a new account is created 
for him in the virtual machine.

By default the account is created with the `useradd` command.

The hook 'add_user' allows an Administrator to modify this proccess and create the user account 
using an alternate method or script.

after_add_user
^^^^^^^^^^^^^^
key: *vma.on_provisioning.after_add_user*

Once the user account has been created, this hook can be used to perform additonal actions related to
setting up the user account within the virtual machine, such as the automatic configuration of an email
client or other similar tasks.

mount_home
^^^^^^^^^^
key: *vma.on_provisioning.mount_home*

By default, QVD mounts the first partition of the device configured with the entry "vma.user.home.drive" 
on the directory "/home" where the user's home directory is created (by the hook 'add_user'). Should this 
partition not exist, it is created on the fly.

With this hook it is possible to change this process so that some other behaviour takes place instead, such
as mounting a "/home" directory from an NFS server.


//[bibliography]
//Bibliography
//============
//The bibliography list is a style of AsciiDoc bulleted list.

//[bibliography]
//- [[[taoup]]] Eric Steven Raymond. 'The Art of Unix
//  Programming'. Addison-Wesley. ISBN 0-13-142901-9.
//- [[[walsh-muellner]]] Norman Walsh & Leonard Muellner.
//  'DocBook - The Definitive Guide'. O'Reilly & Associates. 1999.
//  ISBN 1-56592-580-7.


[glossary]
Glossary
========
//Glossaries are optional. Glossaries entries are an example of a style
//of AsciiDoc labeled lists.

[glossary]

QVD::
  The Quality Virtual Desktop, a set of server components and a client application that provides remote virtual desktop access to users.

QVD Client::
  A modified NX Client capable of connecting to a Virtual Machine running on a QVD Server Node. The client is available for Linux and Windows operating systems.

QVD-DB::
  The QVD database. This is installed on top of a PostgreSQL RDBM Server. All of the server-side components within the QVD infrastructure rely on the database
  to communicate with each other and to implement functionality.

QVD Server Node::
  A host that is running the QVD Server Node components, including the QVD Node daemon, the L7R daemon and the HKD. 
  Usually there are multiple QVD Server Nodes within a typical deployment. The Virtual Machines that the QVD Client accesses
  run on different QVD Server Nodes.

QVD-WAT::
  The QVD Web Administration Tool. This is a web-based GUI that allows an Administrator to configure and monitor the running of the QVD environment.

QVD CLI Administration Utility::
  A Perl script that provides a command line interface with which the QVD environment can be monitored and managed.

L7R::
  The Layer-7 Router which acts as the broker for all QVD Client connections. This is a QVD Server Node daemon. It is responsible for authenticating users
  and routing client requests to the appropriate Server Node running the Virtual Machine for an authenticated user. It also monitors session status.

HKD::
  The House Keeping Daemon is a QVD Server Node daemon. It is responsible for starting and stopping Virtual Machines and for performing virtual machine health
  checking.

QVD Node::
  The QVD Node is a simple QVD Server Node daemon that is used to manage the run states of the L7R and HKD.

VMA::
  The Virtual Machine Agent is a QVD component that runs inside of a virtual machine to facilitate client connectivity to the virtual desktop and that is responsible
  for listening to management requests sent by the HKD. It also provides a number of 'hooks' that allow an Administrator to customize behaviours within the virtual machine.

VMA Hook::
  A facility within the VMA to trigger other functionality (usually through the use of scripts) within the virtual machine, based on particular state changes within the virtual machine.

Virtual Machine::
  A Virtual Machine is a virtualized system running on top of a base Operating System. Usually the virtualized system loads an OSF for the purpose of running a virtual operating system.

OSF::
  An Operating System Flavour is loaded into any number
  of virtual machines on a QVD Server Node in order to serve a virtual desktop to a client. The OSF is usually installed into QVD along with particular runtime parameters
  such as the amount of system memory that should be available to it.

DI::
  A Disk Image is a qcow2 image that has been created as a virtual disk containing an installed operating system. This image is then associated to an OSF.

User::
  Person using the QVD service, usually connected using the QVD Client.

Administrator::
  A user who has permission to access the management platform, usually via the QVD-WAT or through the QVD CLI Administration Utility

Session::
  The period that a user is actually connected to a virtual machine.



//[index]
//Index
//=====
