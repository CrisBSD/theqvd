// -*- Mode: ADoc -*-
How to install QVD 
==================
QVD Team <qvd@theqvd.com>
v3.0.0, May 2011

Introduction
------------

This guide describes how to install and configure the QVD software
step by step. Following it, you will be able to deploy a full VDI
platform.

A good knowledge of Linux and IP network administration will be
required from your part to be able to adapt the sample instructions
given here to your particular scenario. 
Reading the http://theqvd.com/en/documentation/overview/qvd-overview[Technology
Overview] document is also recomended to get some prior familiarity

Special configurations are not described in this guide, but you
will find pointers to other documents describing how to perform them
at the relevant places.

NOTE: This document describes how to install the QVD components that run on
the server side exclusively. In order to install the client, read the
http://theqvd.com/en/documentation/operations/gui-client-user-guide[QVD Client Installation document].



Planning
--------

Before getting hands on installing the software, you should decide the
type of QVD deployment you want to perform based on the following
criteria:

* The installation purpose (do you just want to test the software or
  are you planning to serve virtual desktops to real users?).
* How many concurrent running virtual machines do you want to support.
* The service criticality.
* The hardware or budget available.

If you just want to try the software or want to serve a few users and
the service is not critical you can install all the QVD components in
one single server (we call this the mononode installation).

Otherwise, you will need several servers in order to support the full
load generated by your users virtual machines and to guarantee the
continuity of the service in case of hardware failures (we call this a
QVD farm or a multinode installation).

Following is a detailed description of every component accompanied by a
discussion of the matters you should take into account when deciding
where to place them and how many servers you need to run them and how
big they should be.


QVD Nodes
~~~~~~~~~

These are the servers running the virtual machines and the connection
broker layer.

There should be enough of them to support the peak loads plus an extra
few to overcome hardware failures.

Dimensioning the number of nodes requires some experimentation as it
is very dependant of your users usage patterns (i.e., the applications
they run, the size of the files they open, the ratio of time they
dedicate to use the computer, what they do at certain times of the
day, etc.).

Nonetheless, we'd like to provide some rough numbers in order to guide
you:

* allocate 512MB of RAM per virtual mahine.
* allocate 1 CPU core per every 8 virtual machines.
* Choose a shared storage infrastrucure able to provide 10 iops per virtual machine. 

In order to calculate the global RAM and CPUs required
you should not consider the expected number of concurrent connected
users but the expected number of virtual machines running. Unless you
plan to set policies that shutdown virtual machines when users become
disconnected you should dimensionate your infrastructure to support
all your users virtual machines running at the same time.

The next step, once you know how much memory and how many processor
cores you need, is to decide which kind of servers to use in order to
fullfill that requirements. What is better, a lot of small servers or
just some big ones? Or maybe something in the middle?

A lot of effort has been made into making QVD escalate almost linearly
(it is possible to run more than 500 virtual machines in one server
without performance degradation), so usually the only factors to
consider are the monetary ones: how much will the initial hardware cost
and how much will be the operational costs over time (including power
consumption, air conditioning, support contracts, etc.).

Also consider that one or two extra servers have to be allocated in
order to overcome hardware failures and other unexpected problems.

TIP: RAM is more important than CPU. Having too few CPUs just means
slower operation, your users would notice it but would still be able
to do their work. On the other hand, if the total memory required by
the virtual machines running in some host becomes bigger than the host
RAM it would start using swap space and the virtual machines would
become completely unresponsive from the user point of view.

TIP: If you plan to use the KSM module in order to reduce memory usage,
we recommend you to allocate at least 1/4 or 1/2 of one core to it per
server.

TIP: Keep in mind storage requirements, an standard Linux desktop has 
an average about 10 iops, so you need your shared storage to be able 
to provide it for all your desktops, a good idea can be to use SDD. 


Load Balancer
~~~~~~~~~~~~~

In order to distribute the load among the QVD nodes a frontend load
balancer working at the TCP level is required.

Dedicated hardware or a couple of Linux servers running LVM can be
used to fulfill this requirement.

Its setup is not described in this document. Many documents have been 
written about this issue, some good starting points are the 
http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/[LVS-HOWTO] and the 
http://www.austintek.com/LVS/LVS-HOWTO/mini-HOWTO/LVS-mini-HOWTO.html[LVS-mini-HOWTO]
written both by Joseph Mack, both of these and some more documentation
is available at the http://www.linuxvirtualserver.org/Documents.html[LVS Documentation page]

Database
~~~~~~~~

The QVD software has a PostgreSQL database at its core. All the
configuration and runtime information is stored in the database and if
it fails the full platform will stop working. For that reason, it is
recommended to have the database installed in a high availability
configuration.

Besides that, the actual hardware requirements are very modest, any
moderm server with just two CPU cores and 2GB of RAM will be able to
support the database load.

NOTE: This document does not describe how to setup the database in a
HA configuration, consult Installing a HA Database With PostgreSQL
Lot of documents about this are written on the net, such as http://www.slideshare.net/jessejajti/hadrbdpostgres-postgreswest-08-presentation[Linux-HA + DRBD + PostgreSQL ], http://fanqiang.chinaunix.net/db/psql/2005-09-07/3628.shtml[High Avaibility PostgreSQL HOWTO], an many, many others. 


Storage Service
~~~~~~~~~~~~~~~

An storage service is required in order to share virtual machine disk
images between the nodes. Images are stored as regular files in a
shared file system.

The currently supported services are GFS or OCFS2 on top of some SAN
server (i.e. iSCSI, AoE, etc.) and NFS.

NOTE: This guide does not describe how to set up a storage
server, in order to do it take a look to 
http://nfs.sourceforge.net/nfs-howto/ar01s03.html[Setting up and NFS server] 
and http://nfs.sourceforge.net/nfs-howto/ar01s04.html[Setting up and NFS client]. 
It could be a good idea as well to take a look to the documentation
of specific distros about this issue. 


Web Administration Tool (WAT)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Web Administration Tool allows to manage the full platform using a
friendly and intuitive interface.

As it is not a critical component it can be installed on a dedicated
small server. Another option is to install it on the same servers as
the database (if your corporate security polices allow for this kind
of setup).


Networks
~~~~~~~~

For the mononode installation where all the software components are
installed on the same server, no dedicated networks are required at
all (see mononode network configuration).

Otherwise, in the general case when QVD components are to be
distributed among several machines, a set of networks is required to
interconnect them as follows:

image:../../../images/qvd-network-alt2.png["Network diagrams", width="800"]


Service network
^^^^^^^^^^^^^^^

The service network is where the QVD service is published to the
outside. A load balancer is used to distribute the load among the QVD
nodes.

The load balancer must be connected to the outside.

NOTE: In this document the network 10.0.20.0/24 would be used as the service
network when reference on sample commands.

QVD bus network
^^^^^^^^^^^^^^^

This network is used for comunication between the QVD components and
the virtual machines. An IP address from this network is assigned to
every virtual machine so enough IPs must be reserved in advance for
this network in order to accomodate current and future needs.

If the virtual machines are going to access your intranet or the
internet, it should be connected to the outside through a router or a
firewall (recommended).

QVD automatically allocates IP addresses as new virtual machines are
created on the platform from the assigned segment starting from the
end towards the beggining.

Note that this network should not be used to place extra services for
the VM (i.e. a mailserver or proxy). The virtual machines are
firewalled at their contaning hosts and access to any service on this
network not related to QVD, forbidden.

NOTE: The network 10.1.0.0/16 would be used as the bus network in
sample commands (note that it is a range B network with capacity for
2^16 IPs).

Storage network
^^^^^^^^^^^^^^^

The storage network is a dedicated high capacity network used to
connect the nodes to the storage server.

The database server can be connected to this network or to the bus
network.

NOTE: The network 10.0.21.0/24 would be used as the storage network in
sample commands.

Administration network
^^^^^^^^^^^^^^^^^^^^^^

It is highly recomended to have an additional administration network
accesible only by administrators. The WAT service can be published in
this network.

NOTE: The network 10.0.22.0/24 would be used as the storage network in
sample commands.

.Sample networks summary

****

On the sample commands you will find along this document the following
fictitious network setup is used as reference:

* Machine names
** Load balancer: qvd-lb
** Nodes: qvd-node-0-41
** NFS server: qvd-nfs
** Database server: qvd-db
** WAT server: qvd-wat
** Firewall: qvd-fw
** Adm. network router/firewall: qvd-admfw
** Virtual machines: qvd-vm-214

* Networks:

** Service network
*** network range: 10.0.20.0/24
*** load balancer: 10.0.20.1 (qvd-lb-svc)
*** nodes: 10.0.20.20 - 10.0.20.254 (qvd-node-0-41-svc)
*** iface on servers: eth1
** Storage network
*** network range: 10.0.21.0/24
*** NFS server: 10.0.21.2 (qvd-nfs-stg)
*** database server: 10.0.21.6 (qvd-db-stg)
*** nodes: 10.0.21.20 - 10.0.21.254 (qvd-node-0-41-stg)
*** iface on servers: eth3
** Administration network
*** network range: 10.0.22.0/24
*** router/firewall: 10.0.22.1 (qvd-admfw-adm)
*** NFS server: 10.0.22.2 (qvd-nfs-adm - NFS service not accesible through this network)
*** database server: 10.0.22.6 (qvd-db-adm - database service not accesible through this network)
*** WAT server: 10.0.22.10 (qvd-wat-adm)
*** nodes: 10.0.22.20 - 10.0.22.254 (qvd-node-0-41-adm)
*** iface on servers: eth2
** Bus network
*** network range: 10.1.0.0/16
*** firewall: 10.1.0.1 (qvd-fw-bus)
*** nodes: 10.1.0.20 - 10.1.0.254 (qvd-node-0-41-bus)
*** virtual machines: 10.1.1.0 - 10.1.255.254
*** iface on servers: eth0
*** virtual bridge: qvdbr

* Other services

** DNS service for the virtual machines: 10.2.60.7 (qvd-dns)
** LDAP service for authentication: 10.2.60.8 (qvd-ldap)

****



Extra services
~~~~~~~~~~~~~~

Other extra services as for instance DNS for name resolution, NTP
for time synchronization, syslog for centralized log managment may be
used by the platform.

An LDAP directory can also be used to in order to handle user
authentication.


Software installation and configuration
---------------------------------------

Base setup
~~~~~~~~~~

Once you have decided how to acommodate the QVD components over the
available hardware it is time to install and configure the base OS and
servers.

As a first step, all the servers must be installed with the latest
Ubuntu Server release (currently 11.04). Alternatively, the LTS
release 10.04 can be used, but as virtualization is a relatively
recent addition to Linux, latest kernels with their respective
KVM/qemu userspace tools have fewer bugs and tend to work better.

Configure all the network interfaces as indicated in the Networks
section of the previous chapter, using static IP addresses (a DHCP
service is used internally by the solution and having an external one
in the same network could interfere with its workings).

If you want to use external DNS, NTP, etc. services, configure your
servers to use them.

Having the OpenSSH server running in all your servers is obviously a
must.


Configure QVD repository
~~~~~~~~~~~~~~~~~~~~~~~~

Once your servers are up and running and accesible through the
network, next step is to add the QVD repository to the Ubuntu APT
configuration.

Add the following line to the file /etc/apt/sources.list in all your
servers:

----
deb http://qvd.qindel.com/debian lucid main
----

Then, in order to get the index from the QVD repository downloaded to
your servers, run in every one the following command as root:

----
# apt-get update
----

Install command line administration tools
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Even if not mandatory it is very handy to have the QVD command line
administration tools installed on all the QVD servers (at least on the
QVD nodes and the WAT server).

As root, run the following command:

----
# apt-get install qvd-admin 
----

NOTE: The administration tools will not work until the database
connection has been configured (you can read the WAT setup section
below for instruction on how to do it).


Database server
~~~~~~~~~~~~~~~

In order to get the QVD database running the following actions must be
performed:

* Install the related packages
* Create a new user and database for QVD usage
* Tune database configuration for QVD
* Create database scheme

Detailed steps on how to perform these actions follow:

Database software
^^^^^^^^^^^^^^^^^

Install the package `qvd-db` as follows:

----
# apt-get install qvd-db
----

That installs the PostgreSQL database server and several scripts that
would be used later to create the QVD database.


Create QVD user and database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The following steps have to be performed as user postgres
(automatically created by the PostgreSQL package when it was
installed). Run the following command as root:

----
# su - postgres
----

Then, create the new user `qvduser`:

----
postgres@qvd-db:~$ createuser -P qvduser
Enter password for new role: y-o-u-r--p-a-s-s-w-o-r-d
Enter it again: y-o-u-r--p-a-s-s-w-o-r-d
Shall the new role be a superuser? (y/n) n
Shall the new role be allowed to create databases? (y/n) n
Shall the new role be allowed to create more new roles? (y/n) n
----

And create the new database with `qvduser` set as its owner:

----
postgres@myserver:~$ createdb -O qvduser qvd
----

Tune database configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Three changes are needed on the database configuration:

* Set the transaction isolation level to 'serializable'.
* Allow access to the database from the LAN.
* Allow access for the user `qvduser`.

In order to support concurrent access from all the nodes in the QVD
farm and handle transactions coherently, the transaction isolation
level must be changed from 'read commited' to 'serializable'. This is
a very important step that should not be ommited or your database
would eventually become inconsistent and QVD fail to work.

Edit the file `/etc/postgresql/8.4/main/postgresql.conf` as root and
change the following entries:

----
default_transaction_isolation = 'serializable'
listen_addresses = '*'
----

Edit the file `/etc/postgresql/8.4/main/pg_hba.conf` and add a line
similar to the following one but replacing the sample storage network IP
address for the real one in your installation:

----
host	qvd	qvduser		10.0.21.0/24		md5
----

Once the configuration files are changed, restart the database 
running the following command as root:

----
# /etc/init.d/postgresql-8.4 restart
----

Create database schema
^^^^^^^^^^^^^^^^^^^^^^

In order to create the tables required by QVD, run the following command:

----
# qvd-deploy-db
----

.Resetting QVD
****

If at some point you just want to remove all the nodes, images,
virtual machines, etc. configured in QVD in order to start over (for
instance, if you are testing it), you can use the same command with
the `--force` parameter:

----
# qvd-deploy-db --force
----

Though, note that there is no way to undo that operation other that to
setup eveything back again. Use with care!

****

Storage Server
~~~~~~~~~~~~~~

Describing in detail how to set up the storage server is outside the
scope of this document as it will depend of the kind of technology
selected and the tools supplied by its vendor to administer it.

---
We will limit our discussion here to the layout that must be done 
in order to support the QVD platform correctly.
---


QVD requires several shared directories to store the virtual machine
disk images as follows:

* `images`: operative system disk images are stored here

* `overlays`: this directory contains the kvm disk overlays used to
  track changes between the master images and the virtual machine disk
  images.

* `homes`: this directory contains the disk images used to store the
  user home partition.

* `staging`: this directory is an intermediate upload area used on the
  process of addign new disk images to the platform.

These directories can be all stored on the same shared partition or
distributed among several of them being the only requirement to have
`staging` and `images` on the same partition.

For common installations our recommendation is to have all the
directories in the same partition as it is easier to manage that way.

Other setups can be more suittable for big farms where disk IO can
become a bottleneck and may require to split 'homes' in several shared
partitions or to have `overlays` run on local non-shared
storage. Describing these setups is outside the scope of this
document.

.Sample shared disk
****

On this document explanations and sample commands we would assume our
shared storage is a NFS server running at 10.0.21.2 and exporting the
share `/var/qvd-storage`.

****

QVD Nodes
~~~~~~~~~

The actions required to install a node are as follows:

* Install the software
* Setup the shared storage partition
* Configure a network bridge
* Configure QVD database
* Register the node on the database
* Configure QVD software
* Configure periodic tasks
* Install SSL server certificate
* Starting the software

Installing the software
^^^^^^^^^^^^^^^^^^^^^^^

Install the package qvd-node as follows

----
# apt-get install qvd-node
----

This will install several additional packages besides the Perl modules
used by QVD as iptables, ebtables, qemu-tools, kvm and dnsmasq.

Setup the shared storage partition
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Depending on the type of shared storage you want to use the actual
procedure to set it up may vary. Here we describe how to setup a NFS
server.

The default place where QVD expects to found its shared disk data is
`/var/lib/qvd/storage`. As described in the Shared Storage section, it
uses the subdirectories `images`, `overlays`, `homes` and `staging`
there.

For our sample storage server, to mount the share
`10.0.21.2:/var/qvd-storage` under `/var/lib/qvd/storage` we have to
add the following line to the `/etc/fstab` file in all the QVD Nodes:

----
10.0.21.2:/var/qvd-storage /var/lib/qvd/storage nfs defaults 0 0
----

The required NFS support software is in the package `nfs-common`:

----
# apt-get install nfs-common
----

As it will be pointed below, the machine running the WAT will also
need to have the shared storage mounted in the same way as the QVD
nodes.


Configure the network bridge
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In order to let the virtual machines access the bus network, a virtual
bridge has to be configured on every node to forward the ethernet
traffic between the physical network and the virtual machines.

On our sample configuration we refer to QVD Node `qvd-node-0-41` and
name the bridge `qvdbr` and the bus network is attached to the
interface `eth0`.

To have it configured at server boot time the following entry adapted
to your particular network configuration must be added to the
`/etc/interfaces` file:

----
auto qvdbr
iface qvdbr inet static
  address 10.1.0.41
  netmask 255.255.0.0
  network 10.1.0.0
  pre-up ifconfig eth0 down
  pre-up brctl addbr qvdbr
  pre-up brctl addif qvdbr eth0
  pre-up ifconfig eth0 0.0.0.0
  post-down ifconfig eth0 down
  post-down brctl delif qvdbr eth0
  post-down brctl delbr qvdbr
----

Once the configuration has been updated, restart the node and check
that you can ping some other machine in the bus network, for instance the firewall:

----
root@qvd-node-0-41:/root# ping 10.1.0.1
----

.Bridge interfaces in detail

****

 More information about bridge workings is available from
http://www.linuxfoundation.org/collaborate/workgroups/networking/bridge

This page, even if a bit outdated, contains information specific to
Ubuntu: https://help.ubuntu.com/community/NetworkConnectionBridge

****

Configure QVD database
^^^^^^^^^^^^^^^^^^^^^^

The QVD components running on the nodes get their configuration from
two places, from the file `/etc/qvd/node.conf` and from the database.

The idea is to put as much configuration as possible in the database
leaving in the configuration file exclusively those parameters required
to configure the database connection itself.

The minimal set of entries required are:

* `nodename`: an unique name for every QVD Node, it can be the
  hostname but that condition is not enforced by the software

* `database.host`: the hostname or IP of the server running the PostgreSQL database

* `database.name`: the name of the database used for QVD

* `database.user`: the name of the user owner of the QVD database

* `database.password`: the password used to authenticate the user

For the example configuration used in this document,
`/etc/qvd/node.conf` should contain:

----

nodename = qvd-node-0-41
database.host = 10.0.21.6
database.name = qvd
database.user = qvduser
database.password = y-o-u-r--p-a-s-s-w-o-r-d

----

A configuration file template with some extra fields is installed with
the software at `/usr/share/qvd/config/node.conf`

.Configuration file format

****

QVD configuration files use the Java properties format. See
http://download.oracle.com/javase/6/docs/api/java/util/Properties.html#load%28java.io.Reader%29

****


Register the node in the database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once, the database is configured on the node, we can perform the next
step that would also allow us to check that the database configuration
works right: registering the node in the database.

This just saves in the database the nodename and its IP so that the
rest of the components know where to connect to it.

Do it as follows for every node (obviously using the apropiate
nodename and IP, not the sample ones!):

----
# qvd-admin host add name=qvd-node-0-41 address=10.1.0.41
----

.You can register it from any node
****

Note that you can register the node from any server with the `qvd-admin`
package installed, not just from the one you want to register.

****

Configure QVD software
^^^^^^^^^^^^^^^^^^^^^^

Now that the database is accesible from the node, you can push there
all the configuration. Note that this step does not need to be carried
on for every node but just once because the configuration in the
database is shared by all them.

QVD accepts tens of configuration options, but that does not mean you
should change them at will. Most parameters are there just to let us
tune the platform for you, and the supplied defaults will do fine for
most installations.

There are a few parameters that allow to change the behaviour of the
platform. Those are not documented here, but in the particular
documents describing this alternative behaviours (for instance,
autoprovisioning users and virtual machines from a directory service
on demand).

Then, there are some parameters that must be defined in order to tell
QVD about its environment (for instance, the range of IPs available
for the virtual machines or their default gateway). These parameters
are mandatory and the QVD daemons will refuse to start unless they are
defined. They are as follows:

* `vm.network.start`: first IP of the range allocated for the virtual
  machines on the bus network

* `vm.network.netmask`: size in bits on the bus network address

* `vm.network.gateway`: IP of the firewall on the bus network that
  will be passed by DHCP to the virtual machines

* `vm.network.dns_server`: IP of the DNS service to be configured by
  DHCP on the virtual machines

* `vm.network.bridge`: Name of the bridge interface

These entries can be set on the database using the command `qvd-admin`
available from the `qvd-admin` package as follows:

----
# qvd-admin config set vm.network.start=10.1.0.256
# qvd-admin config set vm.network.netmask=16
# qvd-admin config set vm.network.gateway=10.1.0.1
# qvd-admin config set vm.network.dns_server=10.2.60.7
# qvd-admin config set vm.network.bridge=qvdbr
----

Database configuration can inspected with the following command:

----
# qvd-admin config list
----

Configure periodic tasks
^^^^^^^^^^^^^^^^^^^^^^^^

In some rare cases, the qvd-node running on the QVD Nodes could
terminate leaving some orphan virtual machine process behind (this
could happen for instance, if some administrator kills the daemon
processes).

In order to stop any orphaned virtual machines as soon as possible
(otherwise, they could be started in other node simultaneously, thus
corrupting the disk images), the `qvd-vm-killer` program has to run
every minute.

Add the following line to the root crontab:

----
* * * * *  /usr/bin/qvd-vm-killer.pl
----


Install SSL server certificate
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

QVD uses the SSL protocol to protect the secrecy of the data
transferred between the virtual machines and the client application
and so an X.509 certificate signed by a well respected certification
authority (Verisign, Thawte, etc.) is required.

For testing purposes, a self signed certificate can also be used, but
it will cause a pop-up window to appear to the users warning about its
insecure nature.

Detailed instructions on how to create the private key and get a
certificate for it are available in this HOWTO document from the
OpenSSL website: http://www.openssl.org/docs/HOWTO/certificates.txt

Once you have the X.509 certificate signed by the CA (or a self
signed) and also the original private key, both in PEM format, run
the following command to push them into the QVD database:

----
# qvd-admin.pl config ssl key=path/to/private-key.pem cert=path/to/certificate.pem
----

You should also store the private key and the certificate in some
secure place not accesible through the network (i.e. a CD-ROM stored
securely).

.Creating a self-signed certificate
****

The openssl tool is required for creating a self-signed
certificate. Install it running the following command:

----
# apt-get install openssl
----

The first action you have to perform is to generate the private
key. Run the following command:

----
# openssl genrsa 1024 > private-key.pem
----

Then, a self-signed certificate is created with the following command:

----
# openssl req -new -x509 -nodes -sha1 -days 365 -key private-key.pem > certificate.pem
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
+++-----+++
Country Name (2 letter code) [AU]:NA
State or Province Name (full name) [Some-State:]Sirius
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Sirius Cybernetics Corporation
Organizational Unit Name (eg, section) []:Nutrimatics Unit
Common Name (eg, YOUR name) []:QVD
Email Address []:qvd@scc.sir
----

At this point you will have the server key and the certificate both in
PEM format.

****


Starting the software
^^^^^^^^^^^^^^^^^^^^^

The QVD Node daemon `qvd-node` is configured to be started when the
server boots, but in order to check that everything is configured
correctly, you can start it now with the following command:

----
# /etc/init.d/qvd-node start
----

You can see that it is working looking at the logs:

----
# tail -f /var/log/qvd/qvd.log
----


Web Administration Tool
~~~~~~~~~~~~~~~~~~~~~~~

Installing the WAT is an easy process that only requires installing
the software and setting some configuration options:

Installing the WAT
^^^^^^^^^^^^^^^^^^

In the machine(s) where you want to run the WAT service, run the
following command as root:

----
# apt-get install qvd-wat
----

Configuring the WAT
^^^^^^^^^^^^^^^^^^^

The WAT reads its configuration in the same way as the QVD Node
software from the file `/etc/qvd/node.conf`.

The parameters that have to be configured are the WAT administrator
user and password and those related to the database connection:

* `database.host`: The hostname or IP of the server running the PostgreSQL database

* `database.name`: The name of the database used for QVD

* `database.user`: The name of the user owner of the QVD database

* `database.password`: The password used to authenticate the user

* `wat.admin.login`: Name used to log into the WAT. Note that this
  user is only used for administering the platform through the WAT and
  not related to any of the application users (as the owners of the
  virtual machines).

* `wat.admin.password`: password used to log into the WAT

A sample `/etc/qvd/node.conf` for the WAT follows:

---
# Database connection settings
database.host =  10.0.21.6
database.name = qvd
database.user = qvduser
database.password = y-o-u-r--p-a-s-s-w-o-r-d

# WAT authentication settings.
wat.admin.login = admin
wat.admin.password = a~D~m~1~n
---

If you wish, you can also set the WAT password through the database
with the following command:

----
# qvd-admin.pl config set wat.admin.password=aN-oth3r
----


Testing the WAT
^^^^^^^^^^^^^^^

The WAT has been configured to start automatically when the machine
where it is installed boots, but just to test that you have configured
it correctly, you can start it with the following command:

----
# /etc/init.d/qvd-wat start
----

Then, you will find the WAT listening at port 3000. Just point your
browser there: http://qvd-wat:3000/


